%% josis.tex 1.4   2016-09-15    JoSIS latex template
%------------------------------------------------------------------
% Filename: josis_template.tex
%
% This file is intended as a template for typesetting articles for the
%
%                        Journal of Spatial Information Science.
%
% Please edit this template to generate your own formatted manuscripts
% for submission to JOSIS. See http://josis.org for further details.
%


%%% JOSIS checks in typesetting
%%% * All titles and sections lower case *EXCEPT short title  [ ]
%%% * Remove author postal addresses, only have geographic places and institutions [ ] 
%%% * Consistent use of Section, Figure, Table (capitalized and in full) [ ]
%%% * 10 keywords (and all lower case) [ ]
%%% * Remove all avoidable footnotes [ ]
%%% * Use double quotation marks (``'' not "" or `') [ ]
%%% * Punctuation inside quotations [ ]
%%% * E.g. and i.e. followed by comma [ ]
%%% * cf. followed by tilde [ ]
%%% * Itemize and enumerate correctly punctuated [e.g., "1. x, 2. y, and 3. x." ]
%%% * And/or lists using American English punctuation (e.g., "x, y, and z") [ ] 
%%% * Bibliography (e.g., en-dashes for number ranges, consistent "Proc.~" for Proceedings of..., etc.) []
%%% * Acknowledgment style use section* [ ] 
%%% * et al. no italics, but with dot  [ ] 
%%% * All captions end with full stop  [ ] 
%%% * Table captions under, not over table  [ ]
%%% * Adjust urls with burlalt [ ] 
%%% * Check correct use of hyphens, emdashes, endashes  [ ]
%%% * Perform spell check  [ ] 

%%% JOSIS checks directly before publication 
%%% Check DOI, page numbers on article and web site. [ ]
%%% Update web site with final title, abstract, keywords. [ ] 
%%% Build with distiller for DOI links. [ ]


% Required documentclass definition for JOSIS
\documentclass{josis}
\usepackage{hyperref}
\usepackage[hyphenbreaks]{breakurl}
\usepackage{booktabs}
\usepackage{stmaryrd}
\usepackage[T1]{fontenc}
\usepackage{cite}
\usepackage{subcaption}
\usepackage{longtable}
% Suggested packages for algorithm formatting
\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{algpseudocode}
\usepackage{pythonhighlight}

\usepackage{amssymb,amsmath}
%\usepackage[table]{xcolor}
\usepackage{lastpage}
\renewcommand{\topfraction}{0.9} 
\renewcommand{\textfraction}{0.1}
% Page setup and overhangs
\sloppy
\widowpenalty=10000
\clubpenalty=10000
\hyphenpenalty=75

% Article details for accepted manuscripts will be added by editorial staff
% Omit year if article in press
% Omit number if article under review
\josisdetails{%
   number=2, year=2024, firstpage=1, lastpage=\pageref{LastPage}, 
  doi={IUCP-2024},
  % received={December 24, 2015}, 
   %returned={February 25, 2016},
   %revised={July 13, 2016},
   %accepted={September 5, 2016},
   }

%\newcommand{\mydoi}[1]{\href{http://dx.doi.org/#1}{doi:\protect\detokenize{#1}}}

%\renewcommand{\UrlLeft}{http:\sslash}
%\DeclareUrlCommand\myurl{\def\UrlLeft{}\def\UrlRight{}%
%\urlstyle{tt}}

\urlstyle{rm}
\makeatletter
% Inspired by http://anti.teamidiot.de/nei/2009/09/latex_url_slash_spacingkerning/
% but slightly less kern and shorter underscore
\let\UrlSpecialsOld\UrlSpecials
\def\UrlSpecials{\UrlSpecialsOld\do\/{\Url@slash}\do\_{\Url@underscore}}%
\def\Url@slash{\@ifnextchar/{\kern-.11em\mathchar47\kern-.2em}%
    {\kern-.0em\mathchar47\kern-.08em\penalty\UrlBigBreakPenalty}}
\def\Url@underscore{\nfss@text{\leavevmode \kern.06em\vbox{\hrule\@width.3em}}}
\makeatother

\hypersetup{
colorlinks=true,
linkcolor=black,
citecolor=black,
urlcolor=black
} 

% Add the running author and running title information
\runningauthor{\begin{minipage}{.9\textwidth}\centering Alphonsa Abraham,Navneeth Krishnan,Sidharth V Menon,Vyshnavi Dipu \end{minipage}}
\runningtitle{Creating 2D Occupancy Grids using Overhead Cameras}

% Document begins
\begin{document}
%\setcounter{page}{33}


% Insert your own title
\title{Creating a 2D Occupancy Grid using Overhead cameras }

% Insert your manuscipts authors, affiliations, and addresses
\author{Alphonsa Abraham}
\author{Navneeth Krishnan}
\author{Sidharth V Menon}
\author{Vyshnavi Dipu}\affil{Saintgits Group of Institutions, Kottayam, Kerala}
\date{}
\maketitle
% Add 5-10 keywords for every submission
\keywords{2D occupancy grid, overhead cameras,environmental perception,autonomous systems }
% Add a short abstract of 150-250 words 
\begin{abstract}
This project explores the development and implementation of a 2D occupancy grid mapping system using overhead cameras. The primary goal is to enhance environmental perception and navigation capabilities for autonomous systems in indoor settings. Overhead cameras provide a continuous and wide field of view, capturing dynamic changes in the environment. The system processes the video feed to generate real-time occupancy grids, representing the presence or absence of obstacles within a predefined grid space. Key components include transformation techniques to map camera coordinates to grid coordinates, and real-time updating mechanisms for the occupancy grid. The effectiveness of the approach is validated through a series of experiments in controlled environments, demonstrating its accuracy, responsiveness, and potential applications in robotics, security, and smart infrastructure management. This method offers a cost-effective and scalable solution for dynamic environment monitoring, contributing to advancements in autonomous navigation and intelligent system design.
\end{abstract}
% Your main text begins here. 
\section{Introduction}
 The emergence of intelligent infrastructure and autonomous systems has made sophisticated techniques for navigation and environmental awareness necessary. The 2D occupancy grid mapping system is one such technique that makes use of overhead cameras to track and map changing indoor settings. The goal of this project is to put such a system into operation using Gazebo, a potent robotics simulation tool, and the ROS2 (Robot Operating System 2) framework.\\
A reliable and adaptable framework for creating and integrating intricate robotic systems is offered by ROS2. Its support for modularity, scalability, and real-time connectivity makes it the perfect option for occupancy grid mapping. As the simulation environment, Gazebo provides lifelike models of sensors, robotics, and surroundings. When ROS2 and Gazebo are used together, the occupancy grid mapping system may be thoroughly tested and validated prior deployment in real-world scenarios\\
The first step in the implementation process is to set up the simulation environment in Gazebo. This involves modeling an indoor virtual space and adding both static and dynamic obstacles. The surroundings is continuously captured on video feeds by simulating overhead cameras. After that, these feeds are analyzed by ROS2 nodes, which carry out image processing functions creating a stitched image from 4 overhead cameras and converting into a Occupancy Grid Map. A two-dimensional occupancy grid is mapped each 1s to keep it continuously updated.\\




\section{Libraries Used}
In the project for various tasks, following packages are used.
\begin{python}
    Ubuntu 20.04 Focal Fossy
    Python 3.8
    ROS2 Foxy
    Gazebo Classic
    Turtlebot3
    OpenCV
    Rviz
    
\end{python}
\section{Methodology}
    In this we have main requirements such as a 3D model of an environment with Overhead cameras attached, ROS2 workspace that has turtlebot3, Rviz and Gazebo installed to simulate and access the environment.
    To implement these the following step were used:
\begin{description}
    \item[Launching the environment:] With the required packages installed we are able to deploy and environment with 4 overhead cameras provided by the Industry Mentor and view the topics of these cameras from which the camera feed can be accessed. Launching the environment gives access the the camera feed topics:
    \begin{figure}
        \centering
        \includegraphics[width=0.5\linewidth]{images/TopicList.png}
        \caption{Topics in ROS2}
        \label{fig:enter-label}
    \end{figure}
    \pagebreak
    \item[Collecting Camera Feed:] This step requires the creation of a ROS2 package that has access to the ROS2 topics and can acquire images using the pub-sub model used by ROS2.\\
    The script creates a subscriber to the overhead camera topics and allows us access to each of the cameras.
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.5\linewidth]{images/ImageFeeds.png}
        \caption{Camera feeds from 4 cameras}
        \label{fig:enter-label}
    \end{figure}

    \item[Image Stitching:] We have obtained images from 4 different cameras at 4 different angles, for the preparation of an Occupancy Grid we need to stitch these distinct images together to form a complete top down image that can be mapped. 
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.5\linewidth]{images/StitchedImage.png}
        \caption{Stitched Image}
        \label{fig:enter-label}
    \end{figure}
    \pagebreak
    \item[Color image to Binary image:]To convert an image to an occupancy grid it needs to be a Binary image filled with binary values. This is done using OpenCV's image manipulation function.
    %\item[Classification:] Implementing various Machine Learning Classification models on the document matrix to classify the input text into fake or true news.
    
    
    \item[Generating a 2D Occupancy Grid:] A 2D occupancy grid is generated using ROS2 package OccupancyGrid. This package uses the information in the Binary image to generate a Occupancy grid of the environment in the image. This published occupancy grid topic is then used by the $map_saver$ package to save the Occupancy grid as a .pgm file.
    \item[Occupancy Grid Evaluation:] The Occupancy grid that is generated is then opened in Rviz and we measure the key distances that have been prescribed for evaluation.

\end{description} 
\pagebreak
\section{Implementation}
The 1st step in implementation was launching the model provided by the Industry Mentor and checking for the topics of the Overheard cameras that are part of the model. These topics are necessary as they allow us to access the camera feeds in scripts we use to create the occupancy map.
\begin{figure}[h]
    \centering
    
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/TopicList.png}
        
        \label{fig:subfig1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/ImageFeeds.png}

        \label{fig:subfig2}
    \end{subfigure}
    
    \caption{Image feeds and their topics}
    \label{fig:mainfig}
\end{figure}
These 4 different map feeds are then stitched together using a SIFT algorithm so find the similarities between the 4 images and stitch them accordingly. This helps to create a image that is ordered and looks similar to the environment.\\
The steps are described below:
\begin{itemize}
    \item {\em Feature Detection:} Identify potential keypoints by searching for local maxima and minima in the Difference of Gaussian (DoG) images.Refine the location of keypoints by fitting a quadratic function to nearby data points and rejecting low-contrast keypoints.Assign an orientation to each keypoint based on the local image gradient direction.Generate a descriptor for each keypoint by computing the gradient magnitude and orientation in a local region around the keypoint.
    \item {\em Feature Matching:} Descriptor Matching: Compare descriptors from different images and find pairs of keypoints with the smallest distance between their descriptors.\\
    Use techniques such as the ratio test (e.g., Lowe's ratio test) to filter out false matches.
    \item {\em Homography Estimation:}RANSAC Algorithm: Use the Random Sample Consensus (RANSAC) algorithm to robustly estimate the homography matrix, which helps in dealing with outliers.
    \item {\em Image Warping and Blending: } Perspective Transformation: Apply the homography matrix to transform the image. Blending Techniques: Blend the overlapping regions of the images using techniques like linear blending, multi-band blending, or feathering to create a seamless transition.
    \item {\em Panorama Creation: }Combine the warped images to create a single panoramic image. Ensure that the entire region of interest is covered and that the transition between images is smooth.
\end{itemize}
This completes the stitching process of the image. Now this stitched image is published as a topic and a Occupancy grid is created using OccupancyGrid module from this image. The grid is then saved as a map or .pgm file by using $map_saver$ module.


In the evaluation stage
{\texttt{Rviz}} is used to find the distances between key points in the Grid map .
The key points are highlighted in the image below and the distances are given in the table below:
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{images/KeyPoints.png}
    \caption{Key points}
    \label{fig:enter-label}
\end{figure}
\begin{longtable}{|c|c|}

\hline
Connection & Grid map Length \\ \hline
\endfirsthead
%
\multicolumn{2}{c}%
{{\bfseries Table \thetable\ continued from previous page}} \\
\hline
Connection & Grid map Length \\ \hline
\endhead
%
1          & 4.84m           \\ \hline
2          & 5.63m           \\ \hline
3          & 6.84m           \\ \hline
4          & 7.24m           \\ \hline
5          & 9.28m           \\ \hline
6          & 6.86m           \\ \hline
7          & 5.76m           \\ \hline
8          & 3.73m           \\ \hline
\caption{}
\label{tab:my-table}\\
\end{longtable}

\section{Results \& Discussion}
Measuring the distances between key points on a grid map using RViz is necessary to guarantee accuracy and precision. The reliability of the map can be evaluated by putting markers at known places and contrasting the measured distances on the grid map with actual distances. Disparities point to areas that need to be adjusted or calibrated. This procedure makes sure that the grid map accurately depicts the surroundings, which is important for applications like robotic path planning and autonomous navigation.
The measured points are highlighted in the table below:
\pagebreak
\begin{longtable}{|c|c|}

\hline
Connection & Grid map Length \\ \hline
\endfirsthead
%
\multicolumn{2}{c}%
{{\bfseries Table \thetable\ continued from previous page}} \\
\hline
Connection & Grid map Length \\ \hline
\endhead
%
1          & 4.84m           \\ \hline
2          & 5.63m           \\ \hline
3          & 6.84m           \\ \hline
4          & 7.24m           \\ \hline
5          & 9.28m           \\ \hline
6          & 6.86m           \\ \hline
7          & 5.76m           \\ \hline
8          & 3.73m           \\ \hline
\caption{}
\label{tab:my-table}\\
\end{longtable}
The distances are close to the actual distances the variations are almost completely scaled to the same amount across the key points.
\section{Conclusions}
The use of above cameras to construct a 2D occupancy grid mapping system that is connected with Gazebo and ROS2 shows a notable improvement in autonomous systems' ability to perceive their surroundings. This study effectively used simulation tools and strong image processing techniques to create an accurate, real-time occupancy grid. The system's accuracy and dependability were validated by the RViz study, which also highlighted the system's potential for a wide range of robotics, security, and smart infrastructure applications. Able to map and monitor indoor environments dynamically provides significant advances in intelligent system design and autonomous navigation. To further confirm the system's performance and scalability, future study could include adding more sensors, improving image processing methods, and implementing the system in real-world situations. This initiative establishes a strong basis for future innovation in intelligent environments and autonomous systems.


\section*{Acknowledgments}
We would like to express our heartfelt gratitude and appreciation to Intel$^\copyright$ Corporation for providing an opportunity to this project.First and foremost, we would like to extend our sincere thanks to our team mentor Dr Starlet Ben Alex for his invaluable guidance and constant support throughout the project.We are deeply indebted to our college Saintgits College of Engineering and Technology for providing us with the necessary resources,and sessions on machine learning. We extend our gratitude to all the researchers, scholars, and experts in the field of Robotic Operation System 2, Python programming and Gazebo whose seminal work has paved the way for our project. We acknowledge the mentors, institutional heads, and industrial mentors for their invaluable guidance and support in completing this industrial training under Intel$^\copyright$ -Unnati Programme whose expertise and encouragement have been instrumental in shaping our work.
\cite{*}
\section{References}
\begin{enumerate}
    \item Project Reference, Amartya Saikia, \url{https://github.com/florist-notes/aicore_n/blob/main/notes/int2.MD}\\
    \item ros2, Quigley, M., Gerkey, B., \& Smart, W. D. (2015). \textit{Programming Robots with ROS: A Practical Introduction to the Robot Operating System}. O'Reilly Media, Inc.\\
    \item gazebo,
Koenig, N., \& Howard, A. (2004, September). Design and use paradigms for Gazebo, an open-source multi-robot simulator. \textit{In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, pp. 2149-2154.\\
\item sift,
Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. \textit{International Journal of Computer Vision}, 60(2), 91-110.\\
\item occupancy grid,
Elfes, A. (1989). Using occupancy grids for mobile robot perception and navigation. \textit{Computer}, 22(6), 46-57.\\
\item rviz,
Gossow, D., Kurniawati, H., \& Collett, T. (2011). \textit{RViz: A 3D visualizer for ROS}. [Software]. Available: https://wiki.ros.org/rviz.\\
\item image processing
Gonzalez, R. C., \& Woods, R. E. (2008). \textit{Digital Image Processing} (3rd ed.). Pearson.\\
\item ransac,
Fischler, M. A., \& Bolles, R. C. (1981). Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography. \textit{Communications of the ACM}, 24(6), 381-395.\\
\item feature matching
Bay, H., Ess, A., Tuytelaars, T., \& Van Gool, L. (2008). Speeded-up robust features (SURF). \textit{Computer Vision and Image Understanding}, 110(3), 346-359.\\
\item autonomous navigation
Thrun, S., Burgard, W., \& Fox, D. (2005). \textit{Probabilistic Robotics}. MIT Press.



\end{enumerate}
\end{document}
